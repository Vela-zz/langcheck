{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook to compute the correlation between the `factual_consistency` metric outputs and human annotated consistency scores on benchmark datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAGS-XSUM has 239 data points\n",
      "QAGS-CNN has 235 data points\n"
     ]
    }
   ],
   "source": [
    "# Load the benchmark datasets\n",
    "import json\n",
    "\n",
    "# These files were copied from the UniEval repo\n",
    "# (https://github.com/maszhongming/UniEval/tree/main/reproduce/data/fact), which\n",
    "# is a modified version of the dataset from https://github.com/W4ngatang/qags.\n",
    "qags_xsum_path = 'data/qags_xsum.json'\n",
    "qags_cnndm_path = 'data/qags_cnndm.json'\n",
    "\n",
    "with open(qags_xsum_path) as f:\n",
    "    qags_xsum_data = json.loads(f.read())\n",
    "with open(qags_cnndm_path) as f:\n",
    "    qags_cnndm_data = json.loads(f.read())\n",
    "\n",
    "print(f'QAGS-XSUM has {len(qags_xsum_data)} data points')\n",
    "print(f'QAGS-CNN has {len(qags_cnndm_data)} data points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the generated outputs, sources, and human annotated scores\n",
    "qags_xsum_generated_outputs = [item['system_output'] for item in qags_xsum_data]\n",
    "qags_xsum_sources = [item['source'] for item in qags_xsum_data]\n",
    "qags_xsum_scores = [item['scores']['consistency'] for item in qags_xsum_data]\n",
    "\n",
    "qags_cnndm_generated_outputs = [item['system_output'] for item in qags_cnndm_data]\n",
    "qags_cnndm_sources = [item['source'] for item in qags_cnndm_data]\n",
    "qags_cnndm_scores = [item['scores']['consistency'] for item in qags_cnndm_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "\n",
    "def compute_correlation_values(result, annotated_scores):\n",
    "    '''Function to compute and output the correlation values between the metric\n",
    "    score and the human annotation scores.'''\n",
    "    # Ignore any data points where the evaluator returned `None`. This may happen\n",
    "    # if, for example, the prompt triggers Azure OpenAI's content filter.\n",
    "    result_df = result.to_df()\n",
    "    indices = list(result_df[result_df['metric_value'].notna()].index)\n",
    "    valid_metric_values = [result.metric_values[i] for i in indices]\n",
    "    valid_annotated_scores = [annotated_scores[i] for i in indices]\n",
    "\n",
    "    pearson_corr = pearsonr(valid_metric_values, valid_annotated_scores)[0]\n",
    "    spearman_corr = spearmanr(valid_metric_values, valid_annotated_scores)[0]\n",
    "    kendalltau_corr = kendalltau(valid_metric_values, valid_annotated_scores)[0]\n",
    "\n",
    "    print(f'Pearson correlation = {pearson_corr}')\n",
    "    print(f'Spearman correlation = {spearman_corr}')\n",
    "    print(f'Kendall-Tau correlation = {kendalltau_corr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation = 0.46449467052608684\n",
      "Spearman correlation = 0.48161063910384716\n",
      "Kendall-Tau correlation = 0.39405524553574556\n"
     ]
    }
   ],
   "source": [
    "# Compute the factual consistency scores on QAGS-XSUM using the local (UniEval)\n",
    "# model option and measure various correlations with the human annotated scores\n",
    "from langcheck.metrics import factual_consistency\n",
    "\n",
    "result = factual_consistency(qags_xsum_generated_outputs, qags_xsum_sources)\n",
    "compute_correlation_values(result, qags_xsum_scores)\n",
    "\n",
    "# RUN-DATE: 2023-10-20\n",
    "# Resulting correlation values:\n",
    "#   Pearson correlation = 0.46449467052608684\n",
    "#   Spearman correlation = 0.48161063910384716\n",
    "#   Kendall-Tau correlation = 0.39405524553574556"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the factual consistency scores on QAGS-XSUM using the OpenAI\n",
    "# (gpt-3.5-turbo) model option and measure various correlations with the human\n",
    "# annotated scores\n",
    "from langcheck.metrics import factual_consistency\n",
    "import os\n",
    "from langcheck.metrics.eval_clients import AzureOpenAIEvalClient\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = 'YOUR_AZURE_OPENAI_KEY'\n",
    "os.environ[\"OPENAI_API_VERSION\"] = 'YOUR_OPENAI_API_VERSION'\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = 'YOUR_AZURE_OPENAI_ENDPOINT'\n",
    "client = AzureOpenAIEvalClient(text_model_name='YOUR_DEPLOYMENT_NAME')\n",
    "result = factual_consistency(qags_xsum_generated_outputs,\n",
    "                             qags_xsum_sources,\n",
    "                             eval_model=client)\n",
    "compute_correlation_values(result, qags_xsum_scores)\n",
    "\n",
    "# RUN-DATE: 2023-10-20\n",
    "# Azure OpenAI deployment details:\n",
    "# - Model name: gpt-35-turbo\n",
    "# - Model version: 0613\n",
    "# - API version: 2023-07-01-preview\n",
    "# Resulting correlation values:\n",
    "#   (Computed on 234 examples, since Azure's content filter rejected 5 prompts)\n",
    "#   Pearson correlation = 0.31336126510584367\n",
    "#   Spearman correlation = 0.3170456340335508\n",
    "#   Kendall-Tau correlation = 0.3060538476722336\n",
    "\n",
    "# RUN-DATE: 2023-10-27\n",
    "# Azure OpenAI deployment details:\n",
    "# - Model name: gpt-35-turbo\n",
    "# - Model version: 0613\n",
    "# - API version: 2023-07-01-preview\n",
    "# Resulting correlation values:\n",
    "#   (Computed on 232 examples, since Azure's content filter rejected 7 prompts)\n",
    "#   Pearson correlation = 0.3989261501556993\n",
    "#   Spearman correlation = 0.3968648469619776\n",
    "#   Kendall-Tau correlation = 0.3794472710898086\n",
    "\n",
    "# RUN-DATE: 2023-12-05\n",
    "# Azure OpenAI deployment details:\n",
    "# - Model name: gpt-35-turbo\n",
    "# - Model version: 0613\n",
    "# - API version: 2023-07-01-preview\n",
    "# Resulting correlation values:\n",
    "#   (Computed on 232 examples, since Azure's content filter rejected 7 prompts)\n",
    "#   Pearson correlation = 0.4854067489664019\n",
    "#   Spearman correlation = 0.4981601825844081\n",
    "#   Kendall-Tau correlation = 0.47732669579571085"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation = 0.6582265674108541\n",
      "Spearman correlation = 0.6329252669621304\n",
      "Kendall-Tau correlation = 0.5064287387727447\n"
     ]
    }
   ],
   "source": [
    "# Compute the factual consistency scores on QAGS-CNN using the local (UniEval)\n",
    "# model option and measure various correlations with the human annotated scores\n",
    "from langcheck.metrics import factual_consistency\n",
    "\n",
    "result = factual_consistency(qags_cnndm_generated_outputs, qags_cnndm_sources)\n",
    "compute_correlation_values(result, qags_cnndm_scores)\n",
    "\n",
    "# RUN-DATE: 2023-10-20\n",
    "# Resulting correlation values:\n",
    "#   Pearson correlation = 0.6582265674108541\n",
    "#   Spearman correlation = 0.6329252669621304\n",
    "#   Kendall-Tau correlation = 0.5064287387727447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the factual consistency scores on QAGS-CNN using the OpenAI\n",
    "# (gpt-3.5-turbo) model option and measure various correlations with the human\n",
    "# annotated scores\n",
    "from langcheck.metrics import factual_consistency\n",
    "import os\n",
    "from langcheck.metrics.eval_clients import AzureOpenAIEvalClient\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_KEY\"] = 'YOUR_AZURE_OPENAI_KEY'\n",
    "os.environ[\"OPENAI_API_VERSION\"] = 'YOUR_OPENAI_API_VERSION'\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = 'YOUR_AZURE_OPENAI_ENDPOINT'\n",
    "client = AzureOpenAIEvalClient(text_model_name='YOUR_DEPLOYMENT_NAME')\n",
    "\n",
    "result = factual_consistency(qags_cnndm_generated_outputs,\n",
    "                             qags_cnndm_sources,\n",
    "                             eval_model=client)\n",
    "compute_correlation_values(result, qags_cnndm_scores)\n",
    "\n",
    "# RUN-DATE: 2023-10-20\n",
    "# Azure OpenAI deployment details:\n",
    "# - Model name: gpt-35-turbo\n",
    "# - Model version: 0613\n",
    "# - API version: 2023-07-01-preview\n",
    "# Resulting correlation values:\n",
    "#   (Computed on 217 examples, since Azure's content filter rejected 18 prompts)\n",
    "#   Pearson correlation = 0.41706624916880464\n",
    "#   Spearman correlation = 0.37161022292902374\n",
    "#   Kendall-Tau correlation = 0.31784727756463294\n",
    "\n",
    "# RUN-DATE: 2023-10-27\n",
    "# Azure OpenAI deployment details:\n",
    "# - Model name: gpt-35-turbo\n",
    "# - Model version: 0613\n",
    "# - API version: 2023-07-01-preview\n",
    "# Resulting correlation values:\n",
    "#   (Computed on 210 examples, since Azure's content filter rejected 25 prompts)\n",
    "#   Pearson correlation = 0.6694263995180044\n",
    "#   Spearman correlation = 0.592642518527631\n",
    "#   Kendall-Tau correlation = 0.5244031673150222\n",
    "\n",
    "# RUN-DATE: 2023-12-05\n",
    "# Azure OpenAI deployment details:\n",
    "# - Model name: gpt-35-turbo\n",
    "# - Model version: 0613\n",
    "# - API version: 2023-07-01-preview\n",
    "# Resulting correlation values:\n",
    "#   (Computed on 218 examples, since Azure's content filter rejected 17 prompts)\n",
    "#   Pearson correlation = 0.4967616460898673\n",
    "#   Spearman correlation = 0.45082039363375137\n",
    "#   Kendall-Tau correlation = 0.41909197655671315"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
